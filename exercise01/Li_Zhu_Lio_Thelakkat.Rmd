---
title: "Exercise 1"
author: "Gaby Lio, Shirley Zhu, Jushira Thelakkat, Yuxin Li"
date: "August 10, 2017"
output:
  pdf_document: default
  html_document: default
---

 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
## Probability practice
**Part A: What fraction of people who are truthful clickers answered yes?**

* P(RC)=0.3
*	P(TC)=0.7
*	P(Y|RC)=0.5
*	P(N|RC)=0.5
*	P(Y)=0.65
*	P(N)=0.35

|   |RC|TC|Total|
|:--|--:|--:|--:|
|Y |0.15|0.5|0.65|
|N |0.15|0.2|0.35|
|Total|0.3|0.7|1|

P(Y|TC) = 0.5/0.7 = 71.43%

The fraction of people who answered yes given that they are truthful clickers is 71.43%.

**Part B: Suppose someone tests positive. What is the probability that they have the disease? In light of this calculation, do you envision any problems in implementing a universal testing policy for the disease?**

P(+|D)=0.9993

P(-|Dc)=0.9999

P(D) = 0.000025

P(Dc) = 1-0.000025=0.999975


|   |+|-|Total|
|:--|--:|--:|--:|
|D |2.49825e-05|0.00000018|0.000025|
|Dc|1e-04|0.999875|0.999975|
|Total|0.0001249825|0.999875|1|


P(+,D) = P(+|D) * P(D) = 2.49825e-05

P(-,Dc) =P(-|Dc)* P(Dc) = 0.999875

P(+,Dc) = P(Dc) - P(-,Dc) = 0.999975-0.999875 = 1e-04

P(+) = P(+,D)+P(+,Dc)=2.49825e-05+1e-04=0.0001249825

P(D|+) =P(+,D)/P(+)=2.49825e-05/0.0001249825=0.199888 = **19.99%**

The probability of that someone has the disease given that they test positive is very low, only 19.99%! If they were to implement a universal testing policy for this disease, most people who test positive will not have the disease ~ about 80.01% actually. This would cause chaos, and proves that a universal testing policy for this disease is not recommended.   

## Exploratory Analysis: Green Buildings
Looking through the stat gurus summary about green buildings we realized it was flawed in many ways. The first thing he did wrong was deciding to remove buildings that had less than 10% occupancy from the dataset. In our anlaysis we decided to keep these buildings.

We first wanted to check his claims that rent would be higher for a green building, therefore making a green building more profitable, and convincing his boss to build a green building. We created box plots of green building vs. Rent to assert these claims.
```{r, echo=TRUE}
green_data = read.csv('greenbuildings.csv')

green_data$renovated = as.factor(green_data$renovated)
green_data$class_a = as.factor(green_data$class_a)
green_data$class_b = as.factor(green_data$class_b)
green_data$green_rating = as.factor(green_data$green_rating)
green_data$LEED = as.factor(green_data$LEED)
green_data$Energystar = as.factor(green_data$Energystar)
green_data$net = as.factor(green_data$net)
green_data$amenities = as.factor(green_data$amenities)
green_data$green_rating = as.factor(green_data$green_rating)
```

```{r, echo=TRUE}
plot(green_data$green_rating, green_data$Rent, xlab='GREEN CERTIFIED', ylab='RENT', col='beige')

```

We found that having a green rating only slightly increased the amount of money you would be able to charge tenants for rent. In fact there is barely any variability between the two averages as shown in the box plots above, making the difference not statistically significant. If you go a step further and compare LEED certified vs. not and Energystar certified vs. not you see that in fact LEED energy buildings which are green certified charge a lower rent.
```{r, echo=TRUE}
# Compare RENT values vs. being LEED or ENERGYSTAR CERTIFIED 
par(mfrow = c(1,2))
plot(green_data$LEED, green_data$Rent, xlab='LEED CERTIFED', ylab='RENT', col='pink')
plot(green_data$Energystar, green_data$Rent, xlab='ENERGYSTAR CERTIFED', ylab='RENT', col='burlywood')
```

This already points in the direction of discrediting the gurus claim of a green building being able to charge tenants more for rent. Another major mistake the GURU did was that he only analyzed the data without looking at possible other confouding variables (i.e. age, stories, anemities, net, etc.). When just analyzing Rent vs. green or not green, you are not taking into account other effects variables have. We decided to run a linear regression to see, if holding all other variables constant, being a green building had a signficant impact on rent. 
```{r, echo=TRUE}
lmgreen = lm(green_data$Rent~., data=green_data)
summary(lmgreen)
```

As you can see from the output, when holding all other variables constant, having a green_rating was not significant in affecting Rent at all. Neither was a building having Energystar or LEED certifications (i.e. being green buildings). Other things that had a significant impact on rent included which cluster they belonged in, and each buildings size, age, class, net,amenities, perciption costs, heating days, gas costs, and electricity costs.


We then dived deeper into these insights by seeing if green buildings tended to have amenities thus increasing the price of rent. 
```{r, echo=TRUE}
plot(green_data$green_rating, green_data$amenities, xlab='GREEN RATING', ylab='AMENITIES')
```

As you can see from the plot above, about 70% of green buildings have amentities which means this could be influencing the higher price of rent.

We ran the same type of analysis for the variables net, class a, and class b. You can see that most green buildings are class A (about 80%) and few are Class B (about 20%). This proves that the upcharge in price is likely due to the building being class A and not green. 

```{r, echo=TRUE}
par(mfrow=c(1,2))
plot(green_data$green_rating, green_data$class_a, xlab='GREEN RATING', ylab='CLASS A')
plot(green_data$green_rating, green_data$class_b, xlab='GREEN RATING', ylab='CLASS B')
```


The same goes for the variable net. As you can see below, most green buildings do not have to pay for their own utilities, it is included in the rent costs. Thus adding another confunding variable to why Rent prices could be higher.
```{r, echo=TRUE}
plot(green_data$green_rating, green_data$net, xlab='GREEN', ylab='NET')
```

Lastly, it is told to us in the problem that the building will be 15 stories and will be new. When looking at the relative age of green buildings, they are much lower than non-green buildings.

```{r, echo=TRUE}

plot(green_data$green_rating, green_data$age, xlab='GREEN RATING', ylab='AGE', col='beige')
```

This is becuase green buildings are a newer concept, and did not exist a while ago. From the linear regression output we can see that Age is a significant variable, and if the building is newer, the rent will tend to be higher than if the building was old. Since most green buildings are newer than non-green buildings this could be another factor affecting the rent price.

Overall, there are too many confounding factors that effect the price of Rent. The guru solely basing his argument on the fact that green buildings have a higher rent is a wrong assumption, and therefore invalidates his analysis. 

He also miscalculated the premium one could charge for having a green building. Holding all other variables constant, the premium is only $0.07, much smaller than what the guru proposed per square foot. This means that it would take way longer than 8 years to pay off the building. This calculation although erroneous, still does not matter though becuase the variable was insignficant when other variables were used in the analysis.

Taking all other variables into account, the rent price is higher due to many other variables, and not just the fact that the building is green. If we had more data, such as which location cluster the building would fall into, we may be able to predict if the rent of the building would be higher. Since we only know that it will be new and have 15 stories, there is not much more we can predict and the developer should not listen to the gurus anlaysis.  



### Bootstrapping
```{r}
library(mosaic)
library(quantmod)
library(foreach)
```



```{r}
#Import the ETFs and use getSymboles to get their prices from 2007

mystocks = c("SPY", "TLT", "LQD", "EEM", "VNQ")
myprices = getSymbols(mystocks, from = "2007-01-01")
for(ticker in mystocks) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
```



```{r}
# Combine all the returns into a single matrix

all_returns = cbind(	ClCl(SPYa),
								ClCl(TLTa),
								ClCl(LQDa),
								ClCl(EEMa),
								ClCl(VNQa))
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```




```{r}
# The sample correlation matrix
cor(all_returns)
```



```{r}
boxplot(all_returns, main="Daily Return Distribution by Investment Type", names= c("SPYa","TLTa","LQDa","EEMa","VNQa"))
```

Looking at the boxplots, the safest investments are Investment-grade corporate bonds (LQD), US Treasury bonds (TLT) and US domestic equities (QPY) . Their inter-quartile range (IQR, which captures the middle 50% of the data) is < +/-0.005.

The riskier investments are emerging market equities (EEM) and real estate (VNQ), with IQRs between ~+/- 0.01.There is a tradeoff between return and risk, with the higher risk investments yielding higher potential returns.


```{r}
# Compute the returns from the closing prices
pairs(all_returns)
```

We found out there are outliers in the datapoint,especially in the CLCL data that are affecting the viewing ad interpretation of the graphs. Thus, we removed the extreme outliers.

```{r}
#define a function to remove outliers
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}
```



```{r}
# Compute the returns from the closing prices
n = seq(1,dim(all_returns)[2])
all_clean_returns <- data.frame(0) 
for (i in n){
  y <- remove_outliers(all_returns[,i])
  all_clean_returns = cbind(all_clean_returns,y)
  names(all_clean_returns)[names(all_clean_returns) == 'y'] <- colnames(all_returns)[i]
}
all_clean_returns = as.matrix(na.omit(all_clean_returns[,2:6]))
```


Let's plot the cleaned dataset.

```{r}
pairs(all_clean_returns)
```


By looking at the pairs plot, we can estimate that SPY and TLT are negatively correlated; SPY and EEM, VNQ has a very positive correlation with each other. LQD has a positive correlation with TLT but not much with SPY,EEM or VNQ.

Consider the correlation between SPY and EEM and VNQ, we will pick one out of the three with the least standard deviation(lowest risk). Since SPY and TLT negatively correlated, we will keep both of them in a safe profile, thus, when  SPY goes down, TLT will go up to balance out the risk and vise versa. Since LQD has the lowest standard deviation, LQD will be included in the safe profile as well.


Since the dropped-duplicates dataset is only for plotting purposes, and the outliers should be properly sampled in the future simulations, we will continue to work with the `all_returns` dataframe.



```{r}

# Get the standard deviations and Value at risk for each of the five ETFs for comparision

VaR_all=NULL
mean_all=NULL
sd_all=NULL
par(mfrow=c(3,2))
n_days=20
set.seed(1)
for (j in 1:ncol(all_returns)){
  # Now simulate many different possible years
  sim = foreach(i=1:5000, .combine='rbind') %do% {
    totalwealth = 100000
    weights = 1
    holdings = weights * totalwealth
    wealthtracker = rep(0, n_days) # Set up a placeholder to track total wealth
    for(today in 1:n_days) {
      return.today = resample(all_returns[,j], 1, orig.ids=FALSE)
      holdings = holdings + holdings*return.today
      totalwealth = sum(holdings)
      wealthtracker[today] = totalwealth
    }
    wealthtracker
  }
  
  hist(sim[,n_days]- 100000, main = paste('Histogram for ',colnames(all_returns)[j],sep=" "), xlab = "Returns")
  
  # Calculate 5% value at risk
  VaR_all[j] = quantile(sim[,n_days], 0.05) - 100000
  mean_all[j] = mean(sim[,n_days])
  sd_all[j] = sd(sim[,n_days])
}
```


```{r}
VaR_all

mean_all

sd_all
```


To further support our previous findings, we see that EEM and VNQ have very high standard deviations (standard deviation being a measure or risk) and value at risk values compared to LQD, TLT and SPY. So if an investor is looking for a high return, he/she should invest in these  ETFs which have the highest risk.

We also see that LQD seems to be the ETF with the least value at risk and least standard deviation.
So we will look to give this ETF a relatively higher weight in our safe portfolio.







###Even Split Portfolio

Here we give equal weights of 0.2 each to all the five ETFs.

```{r}
initialwealth=100000
sim_even = foreach(i=1:5000, .combine='rbind') %do% {
totalwealth_even = 100000
n_days = 20
weights_even = c(0.2, 0.2, 0.2, 0.2, 0.2)
holdings_even = weights_even * totalwealth_even
wealthtracker_even = rep(0, n_days) # Set up a placeholder to track total wealth

for(today in 1:n_days) {
return.today = resample(all_returns, 1, orig.ids=FALSE)
holdings_even = holdings_even + holdings_even*return.today
totalwealth_even = sum(holdings_even)
wealthtracker_even[today] = totalwealth_even
}
wealthtracker_even
}

hist(sim_even[,n_days]- 100000, main = "Histogram of returns - Even Split", xlab = "Returns")
abline(v=mean(sim_even[,n_days]- 100000), col="red", lwd=2)
```





###Safer Portfolio

For a safer choice than the even split above, we decided on only investing in the three safest invesements; US domestic markets, US Treasury bonds, and corporate bonds. This will minimize the chance of a big loss, but will also limit potential for large gains.
Historically the bond market has been less vulnerable to price swings or volatility than the stock market.

A safer stock would be the one which does not vary much due to changes in other stocks and has positive returns. In this case, LQD is one such stock with close to zero correlation with all other stocks except TLT. 

Also, among LQD, TLT and SPY, LQD has the least standard deviation (least risk) so we give a higher weight of 0.5 to LQD and lesser weight of 0.2 to SPY.



```{r}
initialwealth=100000
sim_safe = foreach(i=1:5000, .combine='rbind') %do% {
totalwealth_safe = 100000
n_days = 20
weights_safe = c(0.2, 0.3, 0.5, 0, 0)
holdings_safe = weights_safe * totalwealth_safe
wealthtracker_safe = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
return.today = resample(all_returns, 1, orig.ids=FALSE)
holdings_safe = holdings_safe + holdings_safe*return.today
totalwealth_safe = sum(holdings_safe)
wealthtracker_safe[today] = totalwealth_safe
}
wealthtracker_safe
}
hist(sim_safe[,n_days]- 100000, main = "Histogram of returns - Safe", xlab = "Returns")
abline(v=mean(sim_safe[,n_days]- 100000), col="red", lwd=2)
```






###Riskier Portfolio

For the risky portfolio, we decided on investing an even split in the two riskiest investments from above; emerging market equities (EEM) and real estate (VNQ) . This will maximize the chance at a big return, but will also open the possibility for severe losses.



```{r}
initialwealth=100000
sim_risk = foreach(i=1:5000, .combine='rbind') %do% {
totalwealth_risk = 100000
n_days = 20
weights_risk = c(0, 0, 0, 0.5, 0.5)
holdings_risk = weights_risk * totalwealth_risk
wealthtracker_risk = rep(0, n_days) # Set up a placeholder to track total wealth
for(today in 1:n_days) {
return.today = resample(all_returns, 1, orig.ids=FALSE)
holdings_risk = holdings_risk + holdings_risk*return.today
totalwealth_risk = sum(holdings_risk)
wealthtracker_risk[today] = totalwealth_risk
}
wealthtracker_risk
}
hist(sim_risk[,n_days]- 100000, main = "Histogram of returns - Aggressive", xlab = "Returns")
abline(v=mean(sim_risk[,n_days]- 100000), col="red", lwd=2)
```





```{r}
names = c("Even","Safe","Aggresive")
average = c(mean(sim_even[,20]), mean(sim_safe[,20]), mean(sim_risk[,20]))
profit_prob = c(sum(sim_even[,20]>100000)/5000, sum(sim_safe[,20]>100000)/5000, sum(sim_risk[,20]>100000)/5000)
VaR = c((quantile(sim_even[,n_days], 0.05) - 100000), (quantile(sim_safe[,n_days], 0.05) - 100000), (quantile(sim_risk[,n_days], 0.05) - 100000))

data.frame(names, VaR)

```



The aggressive portfolio is definitely the riskiest with the highest absolute value at risk. The largest percentage of the portfolio value that one might lose over a given time period is 13000 dollars for a risky portfolio, to a 5% degree of certainty.

The safe portfolio has the least absolute value at risk. The largest percentage of the portfolio value that one might lose over a given time period is just 2900 dollars for a safe portfolio, to a 5% degree of certainty.



```{r}
data.frame(names, average, profit_prob)
```



Average simulated values of the portfolios and the probability of making a profit can help make investment decisions.

The aggressive portfolio which gave equal weights to the two riskiest ETFs has the highest average return (high risk-high return) but only 52% of portfolios would result in a profit. 

The safe portfolio that gave maximum weight to LQD and lesser weights to TLT and SPY, has least average profit, but 61% of portfolios would result in a profit. 

So the investor faces this risk-return tradeoff at the portfolio level while considering investment decisions.
### Market Segmentation

We decided to use clustering to see if we could find the different market segments for the company. As for the data pre-processing, we did not remove any variables but made sure to center and scale the variables before we ran the K-means regression. We figured that K-means would be the simpliest way to identify different segments in the market through clustering.
```{r,message=FALSE }
library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)

socialmarketing = read.csv('social_marketing.csv', header=TRUE)
dim(socialmarketing)

# Center and scale the data
X = socialmarketing[,-(1:1)]
X = scale(X, center=TRUE, scale=TRUE)
summary(X)
# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
```

First we decided to make a plot of the sum of squares vs. the different choices of K to find the optimal K in which to use in our kmeans ++ model. Using the elbow method we found that 4-6 were the optimal numbers for K.
```{r, echo=TRUE}
### finding the optimal K###
set.seed(2)
mydata <- X
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(mydata,
                                     centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```

We ran a kmeans++ model using k=4, k=5, and k=6. Below you can see the outputs for each one. It turns out the using K=4 and K=5, did not give us enough clusters, and that when we used K=6 we could see distinct clusters that differed from one another.
```{r, echo=TRUE}

### optimal output from above shows wither 3 or 6 for k so lets try both
set.seed(1)
clust5 = kmeanspp(X, k=5, nstart=25)


#cluster 3 centers
clust5$center[1,]*sigma + mu #chatter, photo_sharing, not active
clust5$center[2,]*sigma + mu #cooking, fashion, beauty, chatter, photo sharing
clust5$center[3,]*sigma + mu #chatter, photo sharing, personal fitness, health nutrition
clust5$center[4,]*sigma + mu #chatter, photo sharing, parenting, sports fandom, food, family
clust5$center[5,]*sigma + mu #chatter, photo_sharing, news, politics, travel, 

#checking to see sum of squares
clust5$tot.withinss
clust5$betweenss

options(scipen=999)

set.seed(1)
clust4 = kmeanspp(X, k=4, nstart=25)
#cluster 4  centers
clust4$center[1,]*sigma + mu # food, sports fandom, religion, parenting
clust4$center[2,]*sigma + mu # not active
clust4$center[3,]*sigma + mu #travel, politics, news
clust4$center[4,]*sigma + mu #cooking, fashion, shopping, health_nutrition

#checking to see sum of squares
clust4$tot.withinss
clust4$betweenss

set.seed(1)
clust6 = kmeanspp(X, k=6, nstart=25)
#cluster 6  centers
clust6$center[1,]*sigma + mu #chatter, photo sharing
clust6$center[2,]*sigma + mu #online gaming, college universities
clust6$center[3,]*sigma + mu #health nutrition, personal fitness
clust6$center[4,]*sigma + mu # sports fandom, parenting, religion
clust6$center[5,]*sigma + mu # politics, travel, news
clust6$center[6,]*sigma + mu # fashion, cooking, beauty, photo sharing 


#checking to see sum of squares
clust6$tot.withinss
clust6$betweenss
```
Usin k=6, we found the following clusters to represent the following market segments:

1. Cluster 1
- Focused around those who just used a lot of chatter or photo sharing, and did not really focus on any specific topic when tweeting. They also did not seem to be using twitter a lot since their counts were low in every topic.

2. Cluster 2
- Focused around people who mentioned online gaming or college universities a lot. We figures that this could be a young male population consisting of 16-22 year olds who are active on twitter.

3. Cluster 3
- Focused around people who talked a lot about health nutrition and personal fitness. This customer segment could possibly be young adults or adults who are very into fitness and staying healthy, and who regulary attend the gym and eat nutrious foods. 

4. Cluster 4
- Focused around sports fandom, parenting and religion. This customer segment more liekly than not represents the parents of families who have children, therefore consisting of an older adult crowd.

5. Cluster 5
- Focused around those who mentioned politics, travel, automotive, computers and news. This customer segment probably consits of older men who are educated and probably have more money.

6. Cluster 6
- Focused on cooking, fashion, and beauty. This cusomter segment probably represents mothers or adult/young adult women.

Below we plotted some of the key variables that represent every customer segment using K=6. 
```{r, echo=TRUE}
# qplot is in the ggplot2 library
qplot(chatter, photo_sharing,data=socialmarketing, size=I(3), color=factor(clust6$cluster))
qplot(online_gaming, college_uni,data=socialmarketing, size=I(3), color=factor(clust6$cluster))
qplot(health_nutrition, personal_fitness, data=socialmarketing, size=I(3), color=factor(clust6$cluster))
qplot(religion, sports_fandom, data=socialmarketing, size=I(3), color=factor(clust6$cluster))
qplot(news, politics,data=socialmarketing, size=I(3), color=factor(clust6$cluster))
qplot(beauty, fashion,data=socialmarketing, size=I(3), color=factor(clust6$cluster))


```

