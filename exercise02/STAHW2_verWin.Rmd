---
title: "STA 380 Homework 2_ver.Winnie"
author: "Yuxin Li"
date: "8/15/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Author attribution

```{r}
rm(list=ls())
library(tm) 
library(magrittr)
```

```{r}
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
							
## get a list of author directories
author_dirs = Sys.glob('ReutersC50/C50train/*')

file_list = NULL
#loop through all the documents for each authors
for(author in author_dirs) {
	author_name = substring(author, first=21)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
}
all_docs = lapply(file_list, readerPlain) 

#name all the documents
mynames = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
authorname = file_list %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(.,extract,3)} %>%
	unlist
names(all_docs)= mynames
my_documents = Corpus(VectorSource(all_docs))
```


```{r}
#preprocess/tokenize the corpus
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en")) ##remove stopwords
```

```{r}
## create a doc-term-matrix
DTM_all_doc = DocumentTermMatrix(my_documents)
# inspect(DTM_all_doc[1:10,1:20])
DTM_all_doc = removeSparseTerms(DTM_all_doc, 0.95)

X_train = as.matrix(DTM_all_doc)
X_train = X_train/rowSums(X_train) 
```


```{r}
#read in test data

## get a list of author directories
author_dirs_test = Sys.glob('ReutersC50/C50test/*')

file_list_test = NULL

#loop through all the documents for each authors
for(author in author_dirs) {
	author_name = substring(author, first=21)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list_test = append(file_list_test, files_to_add)
}
all_docs_test = lapply(file_list_test, readerPlain) 

#name all the documents
mynames_test = file_list_test %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
authorname_test = file_list_test %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(.,extract,3)} %>%
	unlist
names(all_docs_test)= mynames_test
my_documents = Corpus(VectorSource(all_docs))
#preprocess/tokenize the corpus
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en")) ##remove stopwords
## create a doc-term-matrix
DTM_all_doc = DocumentTermMatrix(my_documents)
# inspect(DTM_all_doc[1:10,1:20])
DTM_all_doc = removeSparseTerms(DTM_all_doc, 0.95)

X_test = as.matrix(DTM_all_doc)
X_test = X_test/rowSums(X_test)
```

```{r}
library(naivebayes)

model_nB = naive_bayes(x = X_train, y = names(all_docs))

```



